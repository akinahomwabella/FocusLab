"""
Universal Data Loader for Real and Synthetic Behavioral Data

Supports multiple dataset formats and automatically maps features
to the standard format expected by our HMM model.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Tuple, Dict, Optional, List
from dataclasses import dataclass
import json


@dataclass
class DatasetMetadata:
    """Metadata about a loaded dataset"""
    name: str
    source: str  # 'synthetic', 'keystroke', 'cpt', 'driving', etc.
    n_samples: int
    n_users: int
    features: List[str]
    has_ground_truth: bool
    date_range: Optional[Tuple[str, str]] = None


class BehavioralDataLoader:
    """
    Load and preprocess behavioral data from various sources
    
    Supports:
    - Synthetic data (our generator)
    - Keystroke dynamics datasets
    - Continuous Performance Task (CPT) data
    - Driving alertness data
    - Custom CSV formats
    """
    
    STANDARD_FEATURES = ['reaction_time', 'error_rate']
    
    def __init__(self):
        self.metadata = None
        
    def load_synthetic(
        self,
        filepath: str,
        n_samples: Optional[int] = None
    ) -> pd.DataFrame:
        """Load synthetic data generated by our BehavioralDataGenerator"""
        df = pd.read_csv(filepath)
        
        if n_samples:
            df = df.head(n_samples)
        
        # Already in correct format
        self.metadata = DatasetMetadata(
            name="Synthetic Behavioral Data",
            source="synthetic",
            n_samples=len(df),
            n_users=df['user_id'].nunique() if 'user_id' in df.columns else 1,
            features=self.STANDARD_FEATURES,
            has_ground_truth=True,
            date_range=(df['timestamp'].min(), df['timestamp'].max())
        )
        
        return df
    
    def load_keystroke_dynamics(
        self,
        filepath: str,
        time_window: int = 10
    ) -> pd.DataFrame:
        """
        Load keystroke dynamics data and convert to standard format
        
        Expected columns: user_id, timestamp, hold_time, flight_time, errors
        
        Args:
            filepath: Path to keystroke CSV file
            time_window: Rolling window for computing metrics (in keystrokes)
        """
        df = pd.read_csv(filepath)
        
        # Map keystroke features to our standard features
        processed_data = []
        
        # Group by user and session
        for user_id in df['user_id'].unique():
            user_data = df[df['user_id'] == user_id].copy()
            
            # Compute reaction_time from hold_time and flight_time
            user_data['reaction_time'] = (
                user_data['hold_time'] + user_data['flight_time']
            )
            
            # Compute error_rate over rolling window
            user_data['error_rate'] = (
                user_data['errors'].rolling(time_window, min_periods=1).sum() / time_window
            )
            
            # Add derived features
            user_data['reaction_time_rolling_mean'] = (
                user_data['reaction_time'].rolling(time_window, min_periods=1).mean()
            )
            user_data['reaction_time_rolling_std'] = (
                user_data['reaction_time'].rolling(time_window, min_periods=1).std()
            )
            
            # Infer cognitive state (if not provided)
            if 'true_state' not in user_data.columns:
                user_data['inferred_state'] = self._infer_state_from_metrics(
                    user_data['reaction_time'].values,
                    user_data['error_rate'].values
                )
            
            processed_data.append(user_data)
        
        df_processed = pd.concat(processed_data, ignore_index=True)
        
        self.metadata = DatasetMetadata(
            name="Keystroke Dynamics",
            source="keystroke",
            n_samples=len(df_processed),
            n_users=df_processed['user_id'].nunique(),
            features=self.STANDARD_FEATURES,
            has_ground_truth='true_state' in df_processed.columns,
            date_range=(
                df_processed['timestamp'].min(),
                df_processed['timestamp'].max()
            )
        )
        
        return df_processed
    
    def load_cpt_data(
        self,
        filepath: str
    ) -> pd.DataFrame:
        """
        Load Continuous Performance Task data
        
        Expected columns: user_id, trial, response_time, accuracy, stimulus_type
        """
        df = pd.read_csv(filepath)
        
        # Map CPT features to standard format
        df['reaction_time'] = df['response_time']
        df['error_rate'] = 1 - df['accuracy']
        
        # Compute rolling metrics
        window = 20
        df['reaction_time_rolling_mean'] = (
            df.groupby('user_id')['reaction_time']
            .rolling(window, min_periods=1)
            .mean()
            .reset_index(0, drop=True)
        )
        
        # Infer state based on performance degradation
        df['inferred_state'] = self._infer_state_from_cpt(df)
        
        self.metadata = DatasetMetadata(
            name="Continuous Performance Task",
            source="cpt",
            n_samples=len(df),
            n_users=df['user_id'].nunique(),
            features=self.STANDARD_FEATURES,
            has_ground_truth='true_state' in df.columns
        )
        
        return df
    
    def load_driving_alertness(
        self,
        filepath: str
    ) -> pd.DataFrame:
        """
        Load driving/alertness dataset
        
        Expected columns: user_id, timestamp, reaction_time, lane_deviation, event_type
        """
        df = pd.read_csv(filepath)
        
        # Map driving features to standard format
        # reaction_time is already in correct format
        
        # Use lane deviation as proxy for error rate
        df['error_rate'] = (
            df['lane_deviation'] / df['lane_deviation'].max()
        )
        
        # Time-of-day feature (important for fatigue)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['hour'] = df['timestamp'].dt.hour
        df['time_of_day'] = df['hour'] + df['timestamp'].dt.minute / 60
        
        # Infer fatigue based on time and performance
        df['inferred_state'] = self._infer_state_from_driving(df)
        
        self.metadata = DatasetMetadata(
            name="Driving Alertness",
            source="driving",
            n_samples=len(df),
            n_users=df['user_id'].nunique(),
            features=self.STANDARD_FEATURES + ['time_of_day'],
            has_ground_truth='true_state' in df.columns
        )
        
        return df
    
    def load_custom_csv(
        self,
        filepath: str,
        feature_mapping: Dict[str, str],
        user_col: str = 'user_id',
        timestamp_col: str = 'timestamp'
    ) -> pd.DataFrame:
        """
        Load custom CSV with arbitrary column names
        
        Args:
            filepath: Path to CSV file
            feature_mapping: Dict mapping standard features to CSV columns
                Example: {
                    'reaction_time': 'response_latency_ms',
                    'error_rate': 'incorrect_responses_pct'
                }
            user_col: Name of user ID column
            timestamp_col: Name of timestamp column
        """
        df = pd.read_csv(filepath)
        
        # Rename columns to standard format
        rename_dict = {
            user_col: 'user_id',
            timestamp_col: 'timestamp'
        }
        rename_dict.update(feature_mapping)
        
        df = df.rename(columns=rename_dict)
        
        # Ensure required columns exist
        required = ['user_id', 'timestamp'] + self.STANDARD_FEATURES
        missing = set(required) - set(df.columns)
        if missing:
            raise ValueError(f"Missing required columns after mapping: {missing}")
        
        self.metadata = DatasetMetadata(
            name="Custom Dataset",
            source="custom",
            n_samples=len(df),
            n_users=df['user_id'].nunique(),
            features=self.STANDARD_FEATURES,
            has_ground_truth='true_state' in df.columns
        )
        
        return df
    
    def create_hybrid_dataset(
        self,
        real_data: pd.DataFrame,
        synthetic_data: pd.DataFrame,
        ratio: float = 0.5
    ) -> pd.DataFrame:
        """
        Combine real and synthetic data
        
        Args:
            real_data: Real dataset
            synthetic_data: Synthetic dataset
            ratio: Fraction of real data in final dataset (0-1)
        """
        # Use the smaller dataset size as reference
        target_size = min(len(real_data), len(synthetic_data)) * 2
        n_real = int(target_size * ratio)
        n_synthetic = target_size - n_real
        
        # Ensure we don't sample more than available
        n_real = min(n_real, len(real_data))
        n_synthetic = min(n_synthetic, len(synthetic_data))
        
        # Sample from both datasets
        real_sample = real_data.sample(n=n_real, random_state=42)
        synthetic_sample = synthetic_data.sample(n=n_synthetic, random_state=42)
        
        # Add source label
        real_sample['data_source'] = 'real'
        synthetic_sample['data_source'] = 'synthetic'
        
        # Combine
        hybrid = pd.concat([real_sample, synthetic_sample], ignore_index=True)
        hybrid = hybrid.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"Created hybrid dataset:")
        print(f"  Real samples: {n_real} ({ratio*100:.1f}%)")
        print(f"  Synthetic samples: {n_synthetic} ({(1-ratio)*100:.1f}%)")
        print(f"  Total: {len(hybrid)}")
        
        return hybrid
    
    def _infer_state_from_metrics(
        self,
        reaction_times: np.ndarray,
        error_rates: np.ndarray
    ) -> np.ndarray:
        """
        Infer cognitive state from behavioral metrics using thresholds
        
        States: 0=Focused, 1=Fatigued, 2=Distracted
        """
        states = np.zeros(len(reaction_times), dtype=int)
        
        # Compute percentiles for adaptive thresholding
        rt_33 = np.percentile(reaction_times, 33)
        rt_66 = np.percentile(reaction_times, 66)
        er_33 = np.percentile(error_rates, 33)
        er_66 = np.percentile(error_rates, 66)
        
        for i in range(len(reaction_times)):
            rt = reaction_times[i]
            er = error_rates[i]
            
            if rt < rt_33 and er < er_33:
                states[i] = 0  # Focused
            elif rt > rt_66 or er > er_66:
                if er > er_66:
                    states[i] = 2  # Distracted (high errors)
                else:
                    states[i] = 1  # Fatigued (slow but careful)
            else:
                states[i] = 0  # Default to focused
        
        return states
    
    def _infer_state_from_cpt(self, df: pd.DataFrame) -> np.ndarray:
        """Infer state from CPT performance patterns"""
        states = []
        
        for user_id in df['user_id'].unique():
            user_data = df[df['user_id'] == user_id]
            
            # Track performance over time
            rt = user_data['reaction_time'].values
            er = user_data['error_rate'].values
            
            # Detect fatigue as performance degradation
            rt_trend = np.gradient(rt)  # Increasing RT = fatigue
            er_trend = np.gradient(er)  # Increasing errors = distraction
            
            user_states = np.zeros(len(user_data), dtype=int)
            
            for i in range(len(user_data)):
                if rt_trend[i] > 0 and er_trend[i] < 0:
                    user_states[i] = 1  # Fatigued (slow, fewer errors)
                elif er_trend[i] > 0:
                    user_states[i] = 2  # Distracted (more errors)
                else:
                    user_states[i] = 0  # Focused
            
            states.extend(user_states)
        
        return np.array(states)
    
    def _infer_state_from_driving(self, df: pd.DataFrame) -> np.ndarray:
        """Infer state from driving performance and time-of-day"""
        states = np.zeros(len(df), dtype=int)
        
        # Late night/early morning = likely fatigued
        is_night = (df['hour'] >= 22) | (df['hour'] <= 6)
        
        # High error rate = distracted
        high_error = df['error_rate'] > df['error_rate'].quantile(0.66)
        
        # Slow reaction time = fatigued
        slow_rt = df['reaction_time'] > df['reaction_time'].quantile(0.66)
        
        states[is_night & slow_rt] = 1  # Fatigued
        states[high_error & ~is_night] = 2  # Distracted
        states[~is_night & ~high_error & ~slow_rt] = 0  # Focused
        
        return states
    
    def get_statistics(self, df: pd.DataFrame) -> Dict:
        """Get summary statistics of dataset"""
        stats = {
            'n_samples': len(df),
            'n_users': df['user_id'].nunique() if 'user_id' in df.columns else 1,
            'feature_stats': {
                'reaction_time': {
                    'mean': df['reaction_time'].mean(),
                    'std': df['reaction_time'].std(),
                    'min': df['reaction_time'].min(),
                    'max': df['reaction_time'].max(),
                },
                'error_rate': {
                    'mean': df['error_rate'].mean(),
                    'std': df['error_rate'].std(),
                    'min': df['error_rate'].min(),
                    'max': df['error_rate'].max(),
                }
            }
        }
        
        if 'true_state' in df.columns or 'inferred_state' in df.columns:
            state_col = 'true_state' if 'true_state' in df.columns else 'inferred_state'
            stats['state_distribution'] = df[state_col].value_counts().to_dict()
        
        return stats


def load_dataset(
    source: str,
    filepath: str,
    **kwargs
) -> Tuple[pd.DataFrame, DatasetMetadata]:
    """
    Convenience function to load any supported dataset
    
    Args:
        source: Dataset type ('synthetic', 'keystroke', 'cpt', 'driving', 'custom')
        filepath: Path to dataset file
        **kwargs: Additional arguments for specific loaders
    
    Returns:
        (dataframe, metadata)
    """
    loader = BehavioralDataLoader()
    
    if source == 'synthetic':
        df = loader.load_synthetic(filepath)
    elif source == 'keystroke':
        df = loader.load_keystroke_dynamics(filepath, **kwargs)
    elif source == 'cpt':
        df = loader.load_cpt_data(filepath)
    elif source == 'driving':
        df = loader.load_driving_alertness(filepath)
    elif source == 'custom':
        df = loader.load_custom_csv(filepath, **kwargs)
    else:
        raise ValueError(f"Unknown source: {source}")
    
    return df, loader.metadata


if __name__ == "__main__":
    # Example usage
    print("Testing BehavioralDataLoader...")
    
    # Load synthetic data
    loader = BehavioralDataLoader()
    
    # Check if synthetic data exists
    synthetic_path = Path('data/processed/train_data.csv')
    if synthetic_path.exists():
        df = loader.load_synthetic(str(synthetic_path))
        print(f"\nLoaded {loader.metadata.name}")
        print(f"  Samples: {loader.metadata.n_samples}")
        print(f"  Users: {loader.metadata.n_users}")
        
        stats = loader.get_statistics(df)
        print(f"\nDataset Statistics:")
        print(f"  Reaction Time: {stats['feature_stats']['reaction_time']['mean']:.2f} ± "
              f"{stats['feature_stats']['reaction_time']['std']:.2f} ms")
        print(f"  Error Rate: {stats['feature_stats']['error_rate']['mean']:.3f} ± "
              f"{stats['feature_stats']['error_rate']['std']:.3f}")
        
        if 'state_distribution' in stats:
            print(f"\nState Distribution:")
            for state, count in stats['state_distribution'].items():
                print(f"    State {state}: {count} samples")
    else:
        print("No synthetic data found. Run train_hmm.py first.")
